# Predicting Text Readability with BERT [in progress]

<p align="right">
  <img width="600" height="400" src="https://github.com/HeleneFabia/commonlit-readability/blob/main/images/books-g655f0c6ce_1920.jpg">
</p>


[After recently having dived into some common NLP libraries](https://github.com/HeleneFabia/nlp-exploration), I wanted to apply my knowledge with an actual ML project. Kaggle as the obvious choice for that, however I noticed there are very few NLP challenges available, so I decided to go for one that was already closed. Since I did not want to compete, only practice, that was fine.

The [CommonLit Readability](https://www.kaggle.com/c/commonlitreadabilityprize/overview) challenge is about predicting the appropriate reading level of a text excerpt. The purpose behind this is that students learn better if the texts they read are closely matched to their reading abilities. However, commonly used formulas to estimate the readability of a text are not very precise or very costly (and thus not used in a public education context). 

### Data Analysis
See my notebook [here](https://github.com/HeleneFabia/commonlit-readability/blob/main/notebooks/eda.ipynb).

### Modelling
See my notebook [here](https://github.com/HeleneFabia/commonlit-readability/blob/main/notebooks/train_with_bert.ipynb).

